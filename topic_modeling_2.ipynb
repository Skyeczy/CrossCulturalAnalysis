{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/chenzhengyi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os,glob\n",
    "\n",
    "raw_documents = []\n",
    "file_order = []\n",
    "for filename in glob.glob(os.path.join('*.json')):\n",
    "    #print(filename)\n",
    "    with open(filename, 'r') as f:\n",
    "        text = json.load(f)\n",
    "        file_num = filename.split('.')[0]\n",
    "        file_order.append(file_num)\n",
    "        raw_documents.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "print(len(raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "for doc in raw_documents:\n",
    "    data_str = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        data_str += doc[i]['transcript']\n",
    "       \n",
    "    document.append(data_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_data = []\n",
    "#full_data.append(data_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" we're going to Angel Grove not there boy haven't found Krispy Kreme yet. Billy Billy just the two of us\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function to perform lemmatize and stem preprocessing steps on the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['', \"we're\", 'going', 'to', 'Angel', 'Grove', 'not', 'there', 'boy', \"haven't\", 'found', 'Krispy', 'Kreme', 'yet.', 'Billy', 'Billy', 'just', 'the', 'two', 'of', 'us']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['go', 'angel', 'grove', 'haven', 'krispi', 'kreme', 'billi', 'billi']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = document[0]\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))\n",
    "processed_data = preprocess(doc_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words on the Data set\n",
    "\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processed_data = data_str.split(\" \")\n",
    "processed_document = []\n",
    "for doc in document:\n",
    "    processed_doc = preprocess(doc)\n",
    "    processed_document.append(processed_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_document)\n",
    "\n",
    "#count = 0\n",
    "#for k, v in dictionary.iteritems():\n",
    "#    print(k, v)\n",
    "#    count += 1\n",
    "#    if count > 10:\n",
    "#        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out tokens that appear in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary.filter_extremes(no_below=1, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear. Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 0 (\"angel\") appears 1 time.\n",
      "Word 1 (\"billi\") appears 2 time.\n",
      "Word 2 (\"go\") appears 1 time.\n",
      "Word 3 (\"grove\") appears 1 time.\n",
      "Word 4 (\"haven\") appears 1 time.\n",
      "Word 5 (\"kreme\") appears 1 time.\n",
      "Word 6 (\"krispi\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_corpus[0]\n",
    "bow_doc_0 = bow_corpus[0]\n",
    "for i in range(len(bow_doc_0)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
    "                                               dictionary[bow_doc_0[i][0]], \n",
    "bow_doc_0[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Create tf-idf model object using models.TfidfModel on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.26175782322444485),\n",
      " (1, 0.714446803771632),\n",
      " (2, 0.05990451155333907),\n",
      " (3, 0.357223401885816),\n",
      " (4, 0.1861031420804852),\n",
      " (5, 0.357223401885816),\n",
      " (6, 0.357223401885816)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using Bag of Words\n",
    "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore([bow_corpus[1]], num_topics=5, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.000*\"like\" + 0.000*\"play\" + 0.000*\"challeng\" + 0.000*\"game\" + 0.000*\"alphago\" + 0.000*\"board\" + 0.000*\"world\" + 0.000*\"number\" + 0.000*\"atom\" + 0.000*\"pull\"\n",
      "Topic: 1 \n",
      "Words: 0.000*\"board\" + 0.000*\"challeng\" + 0.000*\"play\" + 0.000*\"like\" + 0.000*\"world\" + 0.000*\"number\" + 0.000*\"game\" + 0.000*\"pull\" + 0.000*\"alphago\" + 0.000*\"atom\"\n",
      "Topic: 2 \n",
      "Words: 0.003*\"world\" + 0.002*\"challeng\" + 0.002*\"play\" + 0.002*\"game\" + 0.002*\"alphago\" + 0.002*\"board\" + 0.002*\"number\" + 0.002*\"like\" + 0.001*\"simplest\" + 0.001*\"underway\"\n",
      "Topic: 3 \n",
      "Words: 0.000*\"challeng\" + 0.000*\"number\" + 0.000*\"world\" + 0.000*\"board\" + 0.000*\"like\" + 0.000*\"play\" + 0.000*\"game\" + 0.000*\"alphago\" + 0.000*\"atom\" + 0.000*\"pull\"\n",
      "Topic: 4 \n",
      "Words: 0.001*\"world\" + 0.001*\"like\" + 0.001*\"number\" + 0.001*\"board\" + 0.001*\"alphago\" + 0.000*\"game\" + 0.000*\"play\" + 0.000*\"challeng\" + 0.000*\"pull\" + 0.000*\"long\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.002*\"game\" + 0.001*\"alphago\" + 0.001*\"match\" + 0.001*\"human\" + 0.001*\"think\" + 0.001*\"thing\" + 0.001*\"intellig\" + 0.001*\"work\" + 0.001*\"happen\" + 0.001*\"like\"\n",
      "Topic: 1 Word: 0.001*\"song\" + 0.001*\"bigger\" + 0.001*\"alphago\" + 0.001*\"game\" + 0.001*\"human\" + 0.001*\"like\" + 0.001*\"lanka\" + 0.001*\"match\" + 0.001*\"move\" + 0.001*\"go\"\n",
      "Topic: 2 Word: 0.002*\"alphago\" + 0.001*\"game\" + 0.001*\"play\" + 0.001*\"learn\" + 0.001*\"human\" + 0.001*\"program\" + 0.001*\"robot\" + 0.001*\"intellig\" + 0.001*\"player\" + 0.001*\"googl\"\n",
      "Topic: 3 Word: 0.001*\"know\" + 0.001*\"think\" + 0.001*\"game\" + 0.001*\"insur\" + 0.001*\"patient\" + 0.001*\"alphago\" + 0.001*\"guy\" + 0.001*\"play\" + 0.001*\"place\" + 0.001*\"data\"\n",
      "Topic: 4 Word: 0.002*\"alphago\" + 0.002*\"game\" + 0.002*\"match\" + 0.001*\"compani\" + 0.001*\"surpris\" + 0.001*\"china\" + 0.001*\"robot\" + 0.001*\"googl\" + 0.001*\"program\" + 0.001*\"deepmind\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=5, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "We will check where our test document would be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.983322262763977\t \n",
      "Topic: 0.003*\"world\" + 0.002*\"challeng\" + 0.002*\"play\" + 0.002*\"game\" + 0.002*\"alphago\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9829550981521606\t \n",
      "Topic: 0.002*\"alphago\" + 0.001*\"game\" + 0.001*\"play\" + 0.001*\"learn\" + 0.001*\"human\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.002*\"alphago\" + 0.002*\"game\" + 0.002*\"match\" + 0.001*\"compani\" + 0.001*\"surpris\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model_tfidf.print_topic(4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = ['alphago','googl','game','go']\n",
    "spam_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenzhengyi/opt/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py:821: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bow_corpus)):\n",
    "    lda_model = gensim.models.LdaMulticore([bow_corpus[i]], num_topics=3, id2word=dictionary, passes=2, workers=2)\n",
    "    topic_lst = []\n",
    "    for index, score in sorted(lda_model[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "        #print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))\n",
    "        topic_lst.append(lda_model.print_topic(index, 5))\n",
    "    spam = True\n",
    "    for tgt_word in target_word:\n",
    "        for topic in topic_lst:\n",
    "            if tgt_word in topic:\n",
    "                spam = False\n",
    "                break\n",
    "    if spam:\n",
    "        spam_lst.append(int(file_order[i]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_lst = [int(i) for i in spam_lst]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "filtered_file = pd.read_excel('/Users/chenzhengyi/Desktop/CU_Video_research/English_vedio_summary.xlsx',header = None, index_col = None, dtype={'file_num':str,'descrip':str, 'descrip2':str})[0] \n",
    "wrong_culture_file = pd.read_excel('/Users/chenzhengyi/Desktop/CU_Video_research/English_vedio_summary.xlsx',header = None, sheet_name = 1, index_col = None, dtype={'file_num':str,'descrip':str, 'descrip2':str})[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_file_lst = list(filtered_file)\n",
    "wrong_culture_file_lst = list(wrong_culture_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_to_filter = []\n",
    "success_to_filter = []\n",
    "wrong_filter = []\n",
    "for filtered_spam in spam_lst:\n",
    "    if filtered_spam in filtered_file_lst: #success filter\n",
    "        success_to_filter.append(filtered_spam)\n",
    "    else: #filtered but not spam\n",
    "        wrong_filter.append(filtered_spam)\n",
    "        \n",
    "for filtered_f in filtered_file_lst:\n",
    "    if filtered_f not in spam_lst: # fail to filter\n",
    "        fail_to_filter.append(filtered_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fail_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 121]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 9, 10, 13, 14, 19, 20, 22, 26]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fail_to_filter[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9964248538017273\t \n",
      "Topic: 0.009*\"game\" + 0.005*\"like\" + 0.005*\"time\" + 0.003*\"exist\" + 0.003*\"think\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore([bow_corpus[165]], num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
    "topic_lst = []\n",
    "for index, score in sorted(lda_model[bow_corpus[165]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_order.index('5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_order.index('10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.9916605949401855\t \n",
      "Topic: 0.005*\"game\" + 0.004*\"play\" + 0.004*\"hour\" + 0.003*\"work\" + 0.003*\"control\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = gensim.models.LdaMulticore([bow_corpus[47]], num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
    "topic_lst = []\n",
    "for index, score in sorted(lda_model[bow_corpus[47]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seems that target word 'game' is creating huge problem! remove it from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = ['alphago','googl','go','comput','intellig','machin','match'] #similar to whitelist\n",
    "spam_lst = []\n",
    "filtered_file = pd.read_excel('/Users/chenzhengyi/Desktop/CU_Video_research/English_vedio_summary.xlsx',header = None, index_col = None, dtype={'file_num':str,'descrip':str, 'descrip2':str})[0] \n",
    "wrong_culture_file = pd.read_excel('/Users/chenzhengyi/Desktop/CU_Video_research/English_vedio_summary.xlsx',header = None, sheet_name = 1, index_col = None, dtype={'file_num':str,'descrip':str, 'descrip2':str})[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenzhengyi/opt/anaconda3/lib/python3.7/site-packages/gensim/models/ldamodel.py:821: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(bow_corpus)):\n",
    "    lda_model = gensim.models.LdaMulticore([bow_corpus[i]], num_topics=5, id2word=dictionary, passes=2, workers=2)\n",
    "    topic_lst = []\n",
    "    for index, score in sorted(lda_model[bow_corpus[i]], key=lambda tup: -1*tup[1]):\n",
    "        topic_lst.append(lda_model.print_topic(index, 5))\n",
    "    spam = True\n",
    "    for tgt_word in target_word:\n",
    "        for topic in topic_lst:\n",
    "            if tgt_word in topic:\n",
    "                spam = False\n",
    "                break\n",
    "            \n",
    "    if spam:\n",
    "        spam_lst.append(int(file_order[i]))\n",
    "spam_lst = [int(i) for i in spam_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_to_filter = []\n",
    "success_to_filter = []\n",
    "wrong_filter = []\n",
    "for filtered_spam in spam_lst:\n",
    "    if filtered_spam in filtered_file_lst: #success filter\n",
    "        success_to_filter.append(filtered_spam)\n",
    "    else: #filtered but not spam\n",
    "        wrong_filter.append(filtered_spam)\n",
    "        \n",
    "for filtered_f in filtered_file_lst:\n",
    "    if filtered_f not in spam_lst: #fail to filter\n",
    "        fail_to_filter.append(filtered_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(success_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wrong_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fail_to_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows that the methods successfully filtered out 98 transcripts(documents) and all of them are in the unrelated list. It means that it achieves precision of 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
